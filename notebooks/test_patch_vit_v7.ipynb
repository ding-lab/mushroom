{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b870788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import anndata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import tifffile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomVerticalFlip, Normalize, RandomCrop, Compose\n",
    "from einops import rearrange\n",
    "from kmeans_pytorch import kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8425b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f08f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiplex_imaging_pipeline.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b841849",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = json.load(open('../data/test_registration/HT397B1_v2/registered/metadata.json'))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = sorted(utils.listfiles('/data/estorrs/mushroom/data/test_registration/HT397B1_v2/registered',\n",
    "                     regex='[0-9].h5ad$'))\n",
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030f084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pct_expression = .02\n",
    "pool = []\n",
    "for fp in fps:\n",
    "    a = sc.read_h5ad(fp)\n",
    "    \n",
    "    spot_count = (a.X.toarray()>0).sum(0)\n",
    "    mask = spot_count > pct_expression * a.shape[0]\n",
    "    a = a[:, mask]\n",
    "\n",
    "    pool += a.var.index.to_list()\n",
    "counts = Counter(pool)\n",
    "channels = sorted([c for c, count in counts.items() if count==len(fps)])\n",
    "len(channels), channels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396df8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_to_visium = {}\n",
    "for fp in fps:\n",
    "    sample = fp.split('/')[-1].replace('.h5ad', '')\n",
    "    a = sc.read_h5ad(fp)\n",
    "    label_to_barcode = {i+1:x for i, x in enumerate(a.obs.index)}\n",
    "    barcode_to_label = {v:k for k, v in label_to_barcode.items()}\n",
    "    a.uns['label_to_barcode'] = label_to_barcode\n",
    "    a.uns['barcode_to_label'] = barcode_to_label\n",
    "#     a.obsm['spatial_scaled'] = (a.obsm['spatial'] * scale).astype(np.int32)\n",
    "    \n",
    "    a = a[:, channels]\n",
    "    sc.pp.log1p(a)\n",
    "    \n",
    "    slide_to_visium[sample] = a\n",
    "slide_to_visium.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visium_channels = list(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(slide_to_visium.values()))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = sorted(utils.listfiles('/data/estorrs/mushroom/data/test_registration/HT397B1_v2/registered',\n",
    "                     regex='ome.tiff$'))\n",
    "fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = []\n",
    "for fp in fps:\n",
    "    channels = utils.get_ome_tiff_channels(fp)\n",
    "    channels = [utils.R_CHANNEL_MAPPING.get(c, c) for c in channels]\n",
    "    pool += channels\n",
    "Counter(pool).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d4c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = sorted([c for c, count in Counter(pool).items() if count==len(fps)])\n",
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_to_multiplex = {}\n",
    "for fp in fps:\n",
    "    sample = fp.split('/')[-1].replace('.ome.tiff', '')\n",
    "    cs, img = utils.extract_ome_tiff(fp, as_dict=False)\n",
    "    img = torch.tensor(img)\n",
    "    thumbnail = TF.resize(img, (int(scale * img.shape[-2]), int(scale * img.shape[-1])))\n",
    "    thumbnail = thumbnail.to(torch.float32)\n",
    "    \n",
    "    cs = [utils.R_CHANNEL_MAPPING[c] for c in cs]\n",
    "    idxs = [cs.index(c) for c in channels]\n",
    "    thumbnail = thumbnail[idxs]\n",
    "    \n",
    "    slide_to_multiplex[sample] = thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0946f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplex_channels = list(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_to_data = {k:v for k, v in slide_to_visium.items()}\n",
    "slide_to_data.update(slide_to_multiplex)\n",
    "slide_to_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa885c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_to_dtype = {s:'visium' for s in slide_to_visium.keys()}\n",
    "slide_to_dtype.update({s:'multiplex' for s in slide_to_multiplex.keys()})\n",
    "slide_to_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525afb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = sorted(slide_to_data.keys())\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformArgs:\n",
    "    top_left: tuple\n",
    "    size: tuple\n",
    "    vflip: bool\n",
    "    hflip: bool\n",
    "\n",
    "def format_expression(tiles, adatas, patch_size):\n",
    "    # add batch dim if there is none\n",
    "    if len(tiles.shape) == 2:\n",
    "        tiles = tiles.unsqueeze(0)\n",
    "    if isinstance(adatas, anndata.AnnData):\n",
    "        adatas = [adatas]\n",
    "    \n",
    "    exp_imgs = []\n",
    "    for tile, adata in zip(tiles, adatas):\n",
    "        tile = rearrange(tile, '(ph h) (pw w) -> h w (ph pw)', ph=patch_size, pw=patch_size)\n",
    "        x = torch.unique(tile, dim=-1)\n",
    "\n",
    "        exp = torch.zeros(x.shape[0], x.shape[1], adata.shape[1], dtype=torch.float32)\n",
    "        l2b = adata.uns['label_to_barcode']\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                labels = x[i, j]\n",
    "                labels = labels[labels!=0]\n",
    "                if len(labels):\n",
    "                    barcodes = [l2b[l.item()] for l in labels]\n",
    "                    exp[i, j] = torch.tensor(adata[barcodes].X.mean(0))\n",
    "        exp = rearrange(exp, 'h w c -> c h w')\n",
    "        exp_imgs.append(exp)\n",
    "    \n",
    "    return torch.stack(exp_imgs)\n",
    "\n",
    "def get_slide_to_labeled(slide_to_adata, crop=True, scale=.1):\n",
    "    slide_to_labeled = {}\n",
    "    for s, a in slide_to_adata.items():\n",
    "        a.obsm['spatial_scaled'] = (a.obsm['spatial'] * scale).astype(np.int32)\n",
    "        labeled_locations = np.zeros(\n",
    "            (np.asarray(a.uns['he_rescaled_warped'].shape[:2]) * scale).astype(int), dtype=int)\n",
    "        for barcode, (c, r) in zip(a.obs.index, a.obsm['spatial_scaled']):\n",
    "            labeled_locations[r, c] = a.uns['barcode_to_label'][barcode]\n",
    "\n",
    "        if crop:\n",
    "            min_c, min_r = a.obsm['spatial_scaled'].min(0)\n",
    "            max_c, max_r = a.obsm['spatial_scaled'].max(0)\n",
    "        else:\n",
    "            min_r, min_c = 0, 0\n",
    "            max_r, max_c = labeled_locations.shape\n",
    "        labeled_locations = labeled_locations[min_r:max_r, min_c:max_c]\n",
    "        slide_to_labeled[s] = torch.tensor(labeled_locations)\n",
    "    return slide_to_labeled\n",
    "\n",
    "class TransformVisium(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=(256, 256),\n",
    "        patch_size=32,\n",
    "        normalize=None,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.normalize = normalize if normalize is not None else nn.Identity()\n",
    "\n",
    "    def __call__(self, x, adatas, transform_args=None):\n",
    "        if transform_args is not None:\n",
    "            x = TF.crop(\n",
    "                x,\n",
    "                transform_args.top_left[0],\n",
    "                transform_args.top_left[1],\n",
    "                transform_args.size[0],\n",
    "                transform_args.size[1],\n",
    "            )\n",
    "            if transform_args.hflip:\n",
    "                x = TF.hflip(x)\n",
    "            if transform_args.vflip:\n",
    "                x = TF.vflip(x)\n",
    "\n",
    "        x = format_expression(x, adatas, patch_size=self.patch_size)\n",
    "        x = self.normalize(x)\n",
    "        return x\n",
    "    \n",
    "class TransformMultiplex(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=(256, 256),\n",
    "        normalize=None,\n",
    "    ):\n",
    "        self.size = size\n",
    "\n",
    "        self.normalize = normalize if normalize is not None else nn.Identity()\n",
    "\n",
    "    def __call__(self, x, transform_args=None):\n",
    "        if transform_args is not None:\n",
    "            x = TF.crop(\n",
    "                x,\n",
    "                transform_args.top_left[0],\n",
    "                transform_args.top_left[1],\n",
    "                transform_args.size[0],\n",
    "                transform_args.size[1],\n",
    "            )\n",
    "            if transform_args.hflip:\n",
    "                x = TF.hflip(x)\n",
    "            if transform_args.vflip:\n",
    "                x = TF.vflip(x)\n",
    "        \n",
    "        x = self.normalize(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SlideDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        order,\n",
    "        slide_to_data,\n",
    "        slide_to_dtype,\n",
    "        multiplex_transform=None,\n",
    "        visium_transform=None,\n",
    "        scale=.1,\n",
    "        size=(256, 256),\n",
    "    ):\n",
    "        self.scale = scale\n",
    "        self.size = size\n",
    "        self.slides = order\n",
    "        self.slide_to_dtype = slide_to_dtype\n",
    "        self.dtypes = sorted(set(slide_to_dtype.values()))\n",
    "        \n",
    "        self.slide_to_multiplex = {\n",
    "            s:obj for s, obj in slide_to_data.items()\n",
    "            if slide_to_dtype[s] == 'multiplex'\n",
    "        }\n",
    "        \n",
    "        self.slide_to_visium_adata = {\n",
    "            s:obj for s, obj in slide_to_data.items()\n",
    "            if slide_to_dtype[s] == 'visium'\n",
    "        }\n",
    "        self.slide_to_visium_labeled = get_slide_to_labeled(\n",
    "            self.slide_to_visium_adata, crop=False, scale=scale)\n",
    "        \n",
    "        multiplex_img = next(iter(self.slide_to_multiplex.values())) # (c h w)\n",
    "        visium_img = next(iter(self.slide_to_visium_labeled.values())) # (h w)\n",
    "        assert multiplex_img.shape[-2:] == visium_img.shape[-2:]\n",
    "\n",
    "        self.multiplex_stacked = torch.stack([\n",
    "            self.slide_to_multiplex[s]\n",
    "            for s in self.slides\n",
    "            if s in self.slide_to_multiplex\n",
    "        ]) # (b c h w)\n",
    "        self.visium_stacked = torch.stack([\n",
    "            self.slide_to_visium_labeled[s]\n",
    "            for s in self.slides\n",
    "            if s in self.slide_to_visium_labeled\n",
    "        ]) # (b h w)\n",
    "        \n",
    "        self.dtype_order = torch.tensor([self.dtypes.index(self.slide_to_dtype[s])\n",
    "                            for s in self.slides])\n",
    "        \n",
    "#         index_order, counts = [], {dtype:0 for dtype in self.dtypes}\n",
    "#         for s in self.slides:\n",
    "#             dtype = self.slide_to_dtype[s]\n",
    "#             index_order.append(\n",
    "#                 [self.dtypes.index(dtype), counts[dtype]]\n",
    "#             )\n",
    "#             counts[dtype] += 1\n",
    "#         self.index_order = torch.tensor(index_order, dtype=torch.long)\n",
    "        \n",
    "        self.multiplex_transform = multiplex_transform\n",
    "        self.visium_transform = visium_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.iinfo(np.int64).max # make infinite\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        transform_args = TransformArgs(\n",
    "            top_left = (\n",
    "                np.random.randint(0, self.multiplex_stacked.shape[-2] - self.size[-2]),\n",
    "                np.random.randint(0, self.multiplex_stacked.shape[-1] - self.size[-1])\n",
    "            ),\n",
    "            size=self.size,\n",
    "            vflip=np.random.rand() > .5,\n",
    "            hflip=np.random.rand() > .5,   \n",
    "        )\n",
    "        \n",
    "        multiplex_tiles = self.multiplex_transform(\n",
    "            self.multiplex_stacked,\n",
    "            transform_args=transform_args\n",
    "        )\n",
    "        visium_tiles = self.visium_transform(\n",
    "            self.visium_stacked,\n",
    "            [slide_to_visium[s] for s in self.slides if self.slide_to_dtype[s]=='visium'],\n",
    "            transform_args=transform_args\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'stacked_multiplex': multiplex_tiles, # (b c h w)\n",
    "            'stacked_visium': visium_tiles, # (b c h/ps w/ps)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ce0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "patch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eceda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.vstack([a.X.toarray().mean(0) for a in slide_to_visium.values()]).mean(0)\n",
    "stds = np.vstack([a.X.toarray().std(0) for a in slide_to_visium.values()]).mean(0)\n",
    "normalize = Normalize(means, stds)\n",
    "visium_transform = TransformVisium(normalize=normalize, size=size, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87279b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = torch.cat([x.mean(dim=(-2, -1)).unsqueeze(0) for x in slide_to_multiplex.values()]).mean(0)\n",
    "stds = torch.cat([x.std(dim=(-2, -1)).unsqueeze(0) for x in slide_to_multiplex.values()]).mean(0)\n",
    "normalize = Normalize(means, stds)\n",
    "multiplex_transform = TransformMultiplex(normalize=normalize, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SlideDataset(\n",
    "    order=samples,\n",
    "    slide_to_data=slide_to_data,\n",
    "    slide_to_dtype=slide_to_dtype,\n",
    "    multiplex_transform=multiplex_transform,\n",
    "    visium_transform=visium_transform,\n",
    "    scale=scale,\n",
    "    size=size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588baaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dtype_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b01ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.multiplex_stacked.shape, ds.visium_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64d2c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f22e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d['stacked_multiplex'].shape, d['stacked_visium'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e501dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=len(d['stacked_multiplex']))\n",
    "for ax, img in zip(axs, d['stacked_multiplex']):\n",
    "    ax.imshow(img[multiplex_channels.index('Pan-Cytokeratin')])\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12301743",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=len(d['stacked_visium']))\n",
    "for ax, img in zip(axs, d['stacked_visium']):\n",
    "    ax.imshow(img[list(a.var.index).index('EPCAM')])\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c6bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=16, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl))\n",
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b['stacked_multiplex'].shape, b['stacked_visium'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a899f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ViT(\n",
    "    image_size = (256, 256),\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c20f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.to_patch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ShapeArgs:\n",
    "    patch_size: int\n",
    "    n_channels: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_to_shape_args = {\n",
    "    'multiplex': ShapeArgs(patch_size=32, n_channels=b['stacked_multiplex'].shape[-3]),\n",
    "    'visium': ShapeArgs(patch_size=1, n_channels=b['stacked_visium'].shape[-3])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.slide_mae import SlideMAEV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c83045",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = SlideMAEV3(\n",
    "    encoder=v,\n",
    "    decoder_dim=512,\n",
    "    n_slides=len(samples),\n",
    "    dtypes=ds.dtypes,\n",
    "    dtype_to_shape_args=dtype_to_shape_args,\n",
    "    slide_dtype_order=ds.dtype_order,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04496729",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [\n",
    "    b['stacked_multiplex'].cuda(),\n",
    "    b['stacked_visium'].cuda()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a812fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss, triplet_loss, overall_loss, pixels = mae(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_loss, triplet_loss, overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30843e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recons(pixels, n_cols=6):\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=n_cols)\n",
    "    for i in range(len(ds.dtypes)):\n",
    "        dtype = ds.dtypes[i]\n",
    "        pred_pixels = pixels[i]\n",
    "        args = dtype_to_shape_args[dtype]\n",
    "        pred_pixels = rearrange(pred_pixels, 'b (ph pw) (h w c) -> b c (h ph) (w pw)',\n",
    "                                ph=8, pw=8, c=args.n_channels, h=args.patch_size, w=args.patch_size)\n",
    "\n",
    "        if dtype == 'multiplex':\n",
    "            x = pred_pixels[:n_cols, channels.index('Pan-Cytokeratin')].cpu().detach()\n",
    "            for col in range(n_cols):\n",
    "                axs[0, col].imshow(x[col])\n",
    "                axs[0, col].axis('off')\n",
    "\n",
    "        if dtype == 'visium':\n",
    "            c_idx = list(next(iter(ds.slide_to_visium_adata.values())).var.index).index('EPCAM')\n",
    "            x = pred_pixels[:n_cols, c_idx].cpu().detach()\n",
    "            for col in range(n_cols):\n",
    "                axs[1, col].imshow(x[col])\n",
    "                axs[1, col].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2f4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recons(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ef97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100000\n",
    "lr = 1e-4\n",
    "opt = torch.optim.Adam(mae.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../data/mae_v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79988e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=16, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb190e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, b in enumerate(dl):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    imgs = [\n",
    "        b['stacked_multiplex'].cuda(),\n",
    "        b['stacked_visium'].cuda()\n",
    "    ]\n",
    "    recon_loss, triplet_loss, overall_loss, pixels = mae(imgs)\n",
    "    overall_loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print(i, recon_loss, triplet_loss, overall_loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        plot_recons(pixels)\n",
    "        plt.title('predicted')\n",
    "        plt.show()\n",
    "        \n",
    "    if i % 5000 == 0:\n",
    "        torch.save(mae.state_dict(), f'../data/mae_v7/{i}iter.pt')\n",
    "        \n",
    "    if i == iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(v.state_dict(), f'../data/mae_v3/1500iter.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0589797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1af674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83818fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "\n",
    "normalize = Normalize(means, stds)\n",
    "transform = InferenceTransformVisium(size=(256, 256), patch_size=32, normalize=normalize)\n",
    "inference_ds = InferenceSlideDatasetVisium(slide_to_adata, size=(256, 256), transform=transform, crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dl = DataLoader(inference_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fb60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inference_ds.image_from_tiles(inference_ds.slide_to_tiles[inference_ds.slides[0]],\n",
    "                                  to_expression=True, adata=inference_ds.slide_to_adata['s0'])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[channels.index('EPCAM')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[channels.index('IL7R')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = inference_ds[0]\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "d['img'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5bbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a04622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62da6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ViT(\n",
    "    image_size = 8,\n",
    "    patch_size = 1,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    channels=len(channels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = SlideMAEV2(\n",
    "    encoder = v,\n",
    "    n_slides = len(slide_to_adata),\n",
    "    decoder_dim = 512,      # paper showed good results with just 512\n",
    "    decoder_depth = 6       # anywhere from 1 to 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae.load_state_dict(torch.load('../data/mae_v5/6000iter.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4955f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded_tokens = torch.zeros(len(inference_ds), size[0] // 32, size[1] // 32, v.pos_embedding.shape[-1])\n",
    "all_decoded_tokens = torch.zeros(len(inference_ds), size[0] // 32, size[1] // 32, mae.decoder_dim)\n",
    "all_pred_patches = torch.zeros(len(inference_ds), len(channels), size[0] // 32, size[1] // 32)\n",
    "bs = inference_dl.batch_size\n",
    "with torch.no_grad():\n",
    "    for i, b in enumerate(inference_dl):\n",
    "        x, slide_idx = b['img'], b['slide_idx']\n",
    "        if v.pos_embedding.is_cuda:\n",
    "            x, slide_idx = x.to(v.pos_embedding.device), slide_idx.to(v.pos_embedding.device)\n",
    "        \n",
    "        encoded_tokens = mae.encode(x, slide_idx)\n",
    "        decoded_tokens = mae.decode(encoded_tokens)\n",
    "        pred_pixel_values = mae.to_pixels(decoded_tokens[:, 1:])\n",
    "\n",
    "        encoded_tokens = rearrange(encoded_tokens[:, 1:], 'b (h w) d -> b h w d',\n",
    "                                  h=size[0] // 32, w=size[1] // 32)\n",
    "        decoded_tokens = rearrange(decoded_tokens[:, 1:], 'b (h w) d -> b h w d',\n",
    "                                  h=size[0] // 32, w=size[1] // 32)\n",
    "        pred_patches = rearrange(\n",
    "            pred_pixel_values, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
    "            h=size[0] // 32, w=size[0] // 32, p1=1, p2=1, c=len(channels))\n",
    "        \n",
    "        all_encoded_tokens[i * bs:(i + 1) * bs] = encoded_tokens.cpu().detach()\n",
    "        all_decoded_tokens[i * bs:(i + 1) * bs] = decoded_tokens.cpu().detach()\n",
    "        all_pred_patches[i * bs:(i + 1) * bs] = pred_patches.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07942235",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded_tokens.shape, all_decoded_tokens.shape, all_pred_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede21ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inference_ds.slide_from_tiles(all_pred_patches, 0, size=all_pred_patches.shape[-2:])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48862744",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = [\n",
    "    'EPCAM', 'KRT18',\n",
    "    'IL7R',\n",
    "    'BGN', 'SPARC', 'VIM',\n",
    "]\n",
    "n_cols = 2\n",
    "n_rows = len(genes) // n_cols + 1\n",
    "rc = [(i, j) for i in range(n_rows) for j in range(n_cols)]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=n_cols, nrows=n_rows)\n",
    "rc = [(i, j) for i in range(n_rows) for j in range(n_cols)]\n",
    "for (row_idx, col_idx), c in zip(rc, genes):\n",
    "    ax = axs[row_idx, col_idx]\n",
    "    ax.imshow(x[channels.index(c)])\n",
    "    ax.axis('off')\n",
    "    ax.set_title(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ace8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing regressing out tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e344820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = sm.datasets.scotland.load()\n",
    "# data.exog = sm.add_constant(data.exog)\n",
    "\n",
    "# gamma_model = sm.GLM(data.endog, data.exog, family=sm.families.Gamma())\n",
    "# gamma_results = gamma_model.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeaf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ec3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_encoded_tokens.clone()\n",
    "x = rearrange(x, 'n h w d -> (n h w) d')\n",
    "x = x.numpy()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e01b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = torch.zeros(all_encoded_tokens.shape[:-1], dtype=torch.long).unsqueeze(-1)\n",
    "idx_to_var = [(r, c) for r in range(regressors.shape[1]) for c in range(regressors.shape[2])]\n",
    "var_to_idx = {v:i for i, v in enumerate(idx_to_var)}\n",
    "for i, (slide_idx, row_idx, col_idx) in enumerate(inference_ds.idx_to_coord):\n",
    "    for r in range(regressors.shape[1]):\n",
    "        for c in range(regressors.shape[2]):\n",
    "            regressors[i, r, c] = var_to_idx[(r, c)]\n",
    "target = rearrange(regressors, 'n h w d -> (n h w) d').squeeze()\n",
    "target = torch.nn.functional.one_hot(target).numpy()\n",
    "# target = target.numpy()\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs = np.random.choice(np.arange(target.shape[0]), size=1000, replace=False)\n",
    "# x = x[idxs]\n",
    "# target = target[idxs]\n",
    "# x.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = LinearRegression()\n",
    "# lm.fit(x, target)\n",
    "# residuals = lm.predict(x) - target\n",
    "# residuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(target, x)\n",
    "residuals = lm.predict(target) - x\n",
    "residuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c7e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc45a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(residuals)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=20)\n",
    "cluster_ids = clusterer.fit_transform(x.numpy())\n",
    "cluster_ids = torch.tensor(cluster_ids.argmin(1))\n",
    "cluster_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_imgs = rearrange(cluster_ids, '(n h w) -> n 1 h w',\n",
    "                        n=all_encoded_tokens.shape[0], h=all_encoded_tokens.shape[1], w=all_encoded_tokens.shape[2])\n",
    "labeled_img = inference_ds.slide_from_tiles(\n",
    "    cluster_imgs, 0, size=(cluster_imgs.shape[-2], cluster_imgs.shape[-1])).squeeze().to(torch.long)\n",
    "\n",
    "stacked_labeled = []\n",
    "for i in range(len(slide_to_adata)):\n",
    "    stacked_labeled.append(inference_ds.slide_from_tiles(\n",
    "        cluster_imgs, i, size=(cluster_imgs.shape[-2], cluster_imgs.shape[-1])).squeeze().to(torch.long))\n",
    "stacked_labeled = torch.stack(stacked_labeled)\n",
    "stacked_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b787dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('tab20')\n",
    "for i, labeled in enumerate(stacked_labeled):\n",
    "    plt.imshow(display_labeled_as_rgb(labeled, cmap=cmap))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d701c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad870a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56631c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_labeled = torch.zeros(338, 8, 8, 1, dtype=torch.long)\n",
    "idx_to_str = {}\n",
    "idx = 0\n",
    "for i, (slide_idx, row_idx, col_idx) in enumerate(inference_ds.idx_to_coord):\n",
    "    for r in range(labeled.shape[1]):\n",
    "        for c in range(labeled.shape[2]):\n",
    "            idx_to_str[idx] = f'slide{slide_idx}_row{row_idx}_col{col_idx}_{r}_{c}'\n",
    "            labeled[i, r, c] = idx\n",
    "            idx += 1\n",
    "labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8fbe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f02d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = inference_ds.slide_to_tiles['s0']\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = inference_ds.slide_to_labeled['s0']\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4df11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = slide_to_adata['s3']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bdbbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.pp.calculate_qc_metrics(a, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac14f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(a.obs['n_genes_by_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e68b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(a.obs['n_genes_by_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ccddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de123c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66535b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf82bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_encoded_tokens.clone()\n",
    "x = rearrange(x, 'n h w d -> (n h w) d')\n",
    "# x /= x.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clusterer = KMeans(n_clusters=20)\n",
    "cluster_ids = clusterer.fit_transform(x.numpy())\n",
    "cluster_ids = torch.tensor(cluster_ids.argmin(1))\n",
    "cluster_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_clusters = 20\n",
    "# cluster_ids, cluster_centers = kmeans(\n",
    "#     X=x, num_clusters=num_clusters, distance='euclidean', device=torch.device('cuda:1'), tol=1.,\n",
    "# )\n",
    "# cluster_ids = cluster_ids.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb52d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_imgs = rearrange(cluster_ids, '(n h w) -> n 1 h w',\n",
    "                        n=all_encoded_tokens.shape[0], h=all_encoded_tokens.shape[1], w=all_encoded_tokens.shape[2])\n",
    "labeled_img = inference_ds.slide_from_tiles(\n",
    "    cluster_imgs, 0, size=(cluster_imgs.shape[-2], cluster_imgs.shape[-1])).squeeze().to(torch.long)\n",
    "labeled_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_labeled_as_rgb(labeled, cmap=None):\n",
    "    if isinstance(labeled, torch.Tensor):\n",
    "        labeled = labeled.numpy()\n",
    "    cmap = sns.color_palette() if cmap is None else cmap\n",
    "    labels = sorted(np.unique(labeled))\n",
    "    if len(cmap) < len(labels):\n",
    "        raise RuntimeError('cmap is too small')\n",
    "    new = np.zeros((labeled.shape[0], labeled.shape[1], 3))\n",
    "    for l in labels:\n",
    "        c = cmap[l]\n",
    "        new[labeled==l] = c\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_labeled = []\n",
    "for i in range(len(slide_to_adata)):\n",
    "    stacked_labeled.append(inference_ds.slide_from_tiles(\n",
    "        cluster_imgs, i, size=(cluster_imgs.shape[-2], cluster_imgs.shape[-1])).squeeze().to(torch.long))\n",
    "stacked_labeled = torch.stack(stacked_labeled)\n",
    "stacked_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('tab20')\n",
    "for i, labeled in enumerate(stacked_labeled):\n",
    "    plt.imshow(display_labeled_as_rgb(labeled, cmap=cmap))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe5a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ebf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae83844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa3f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7e5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06787f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df1ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e442bff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b96237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96f967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11083140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d30d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170922f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb76a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe50aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3aaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967c0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e4a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fcb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = inference_ds.slide_to_labeled['s0'].shape\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = torch.arange(shape[0] * shape[1])\n",
    "labeled = rearrange(labeled, '(h w) -> h w', h=shape[0], w=shape[1])\n",
    "plt.imshow(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = inference_ds.to_tiles(labeled.unsqueeze(0))\n",
    "tiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tiles[6, 8, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = inference_ds.image_from_tiles(tiles).squeeze()\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "338, 8, 8, 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = torch.zeros(338, 8, 8, 1, dtype=torch.long)\n",
    "idx_to_str = {}\n",
    "idx = 0\n",
    "for i, (slide_idx, row_idx, col_idx) in enumerate(inference_ds.idx_to_coord):\n",
    "    for r in range(labeled.shape[1]):\n",
    "        for c in range(labeled.shape[2]):\n",
    "            idx_to_str[idx] = f'slide{slide_idx}_row{row_idx}_col{col_idx}_{r}_{c}'\n",
    "            labeled[i, r, c] = idx\n",
    "            idx += 1\n",
    "labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35117e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_str[labeled[10, 1, 1, 0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_cluster_labeled = rearrange(labeled, 'n h w d -> (n h w) d')\n",
    "pre_cluster_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_cluster_labeled = pre_cluster_labeled.squeeze()\n",
    "post_cluster_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfd27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_cluster_image = rearrange(post_cluster_labeled, '(n h w) -> n 1 h w',\n",
    "                        n=all_encoded_tokens.shape[0], h=all_encoded_tokens.shape[1], w=all_encoded_tokens.shape[2])\n",
    "post_cluster_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_from_tiles_img = inference_ds.slide_from_tiles(\n",
    "    post_cluster_image, 0, size=(post_cluster_image.shape[-2], post_cluster_image.shape[-1])).squeeze().to(torch.long)\n",
    "slide_from_tiles_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc252dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = slide_from_tiles_img[0, 0].item()\n",
    "idx_to_str[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = slide_from_tiles_img[4, 0].item()\n",
    "idx_to_str[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beea875",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = slide_from_tiles_img[33, 44].item()\n",
    "idx_to_str[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "8 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891ab11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9150a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf27f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = inference_ds.slide_to_tiles['s0']\n",
    "tiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(len(inference_ds.idx_to_coord), 8, 8, dtype=torch.bool)\n",
    "for i, (slide_idx, row_idx, col_idx) in enumerate(inference_ds.idx_to_coord):\n",
    "    slide = inference_ds.slides[slide_idx]\n",
    "    labeled_tile = inference_ds.slide_to_tiles[slide][row_idx, col_idx, 0]\n",
    "    labeled_tile = rearrange(labeled_tile, '(ph h) (pw w) -> h w (ph pw)', ph=32, pw=32)\n",
    "    mask[i] = labeled_tile.sum(dim=-1) > 0\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1ad31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# slide_to_hulls = {}\n",
    "# for slide in inference_ds.slides:\n",
    "#     x = inference_ds.image_from_tiles(inference_ds.slide_to_tiles[slide]).squeeze()\n",
    "#     z = 52 / x.shape[0]\n",
    "#     pts = (x!=0).argwhere().to(torch.float32)\n",
    "#     pts *= z\n",
    "#     pts = pts.to(torch.long)\n",
    "    \n",
    "#     mask = np.zeros((52, 52))\n",
    "#     for r, c in pts:\n",
    "#         mask[r, c] = True\n",
    "#     slide_to_hulls[slide] = mask\n",
    "# #     hull = convex_hull_image(mask)\n",
    "# #     slide_to_hulls[slide] = hull\n",
    "# for slide, hull in slide_to_hulls.items():\n",
    "#     plt.imshow(hull)\n",
    "#     plt.title(slide)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1899092",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz = mask.unsqueeze(1)\n",
    "zzz_img = inference_ds.slide_from_tiles(\n",
    "    zzz, 0, size=(zzz.shape[-2], zzz.shape[-1])).squeeze().to(torch.long)\n",
    "zzz_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe76b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(zzz_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_encoded_tokens.clone()\n",
    "x[~mask] = torch.zeros(x.shape[-1])\n",
    "x = rearrange(x, 'n h w d -> (n h w) d')\n",
    "x /= x.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb26497",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x.sum(-1)==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75003944",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d748447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clusterer = KMeans(n_clusters=10)\n",
    "cluster_ids = clusterer.fit_transform(x.numpy())\n",
    "cluster_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfed402",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = cluster_ids.argmin(1)\n",
    "np.unique(cluster_ids, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84091e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = torch.tensor(cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_clusters = 5\n",
    "# cluster_ids, cluster_centers = kmeans(\n",
    "#     X=x, num_clusters=num_clusters, distance='euclidean', device=torch.device('cuda:1'), tol=1.,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_ids, cluster_centers = cluster_ids.cpu().detach(), cluster_centers.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85521438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_ids.shape, cluster_centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b225271",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_imgs = rearrange(cluster_ids, '(n h w) -> n 1 h w',\n",
    "                        n=all_encoded_tokens.shape[0], h=all_encoded_tokens.shape[1], w=all_encoded_tokens.shape[2])\n",
    "cluster_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_img = inference_ds.slide_from_tiles(\n",
    "    cluster_imgs, 0, size=(cluster_imgs.shape[-2], cluster_imgs.shape[-1])).squeeze().to(torch.long)\n",
    "labeled_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39090b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_labeled_as_rgb(labeled, cmap=None):\n",
    "    if isinstance(labeled, torch.Tensor):\n",
    "        labeled = labeled.numpy()\n",
    "    cmap = sns.color_palette() if cmap is None else cmap\n",
    "    labels = sorted(np.unique(labeled))\n",
    "    if len(cmap) < len(labels):\n",
    "        raise RuntimeError('cmap is too small')\n",
    "    new = np.zeros((labeled.shape[0], labeled.shape[1], 3))\n",
    "    for l in labels:\n",
    "        c = cmap[l]\n",
    "        new[labeled==l] = c\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97220f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = sns.color_palette('tab20') + sns.color_palette('tab20b') + sns.color_palette('tab20c')\n",
    "cmap = sns.color_palette('tab20')\n",
    "plt.imshow(display_labeled_as_rgb(labeled_img, cmap=cmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd249db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_labeled = []\n",
    "for i in range(len(slide_to_adata)):\n",
    "    stacked_labeled.append(inference_ds.slide_from_tiles(\n",
    "        cluster_imgs, i, size=(cluster_imgs.shape[-2], cluster_imgs.shape[-1])).squeeze().to(torch.long))\n",
    "stacked_labeled = torch.stack(stacked_labeled)\n",
    "stacked_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef110fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('tab20')\n",
    "for i, labeled in enumerate(stacked_labeled):\n",
    "    plt.imshow(display_labeled_as_rgb(labeled, cmap=cmap))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(slide_to_adata['s0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(slide_to_adata['s3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(slide_to_adata['s3'], color='EPCAM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e7fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
